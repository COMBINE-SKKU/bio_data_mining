{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Before jumping to the code implementation\n",
    "***\n",
    "- Git을 사용하지 않고 해당 ipynb파일만 다운받고 싶으면 wget이나 curl을 사용하여도 좋습니다. \n",
    "    - wget https://raw.githubusercontent.com/COMBINE-SKKU/bio_data_mining/master/Lecture6-EnsembleMethods-Adaboost_GradientBoosting.ipynb\n",
    "    - curl https://raw.githubusercontent.com/COMBINE-SKKU/bio_data_mining/master/Lecture6-EnsembleMethods-Adaboost_GradientBoosting.ipynb --output Lecture6-EnsembleMethods-Adaboost_GradientBoosting.ipynb\n",
    "<br><br><br>    \n",
    "- 동영상과 같이 git clone을 사용하고 싶은 학생이 주의하여야 할 점은 git은 원칙적으로 이미 존재하는 디렉토리에는 clone을 하지 않으므로 디렉토리 (예를 들자면 /Documents/BioDataMining)를 지워주고 실행하여야 합니다. \n",
    "- Open the terminal and make a directory dedicated for this class code implementation (e.g., mkdir ~/Documents/BioDataMining)\n",
    "- Install Git (https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)\n",
    "- Go to the directory for this class, and type: git clone https://github.com/COMBINE-SKKU/bio_data_mining.git\n",
    "- If you are a newbie for programming a python and jupyter notebook, please install anaconda \n",
    "    - Window: https://problemsolvingwithpython.com/01-Orientation/01.03-Installing-Anaconda-on-Windows/\n",
    "    - Mac: https://problemsolvingwithpython.com/01-Orientation/01.04-Installing-Anaconda-on-MacOS/\n",
    "    - Linux: https://problemsolvingwithpython.com/01-Orientation/01.05-Installing-Anaconda-on-Linux/\n",
    "- And learn how to open a jupyter notebook (https://www.youtube.com/watch?v=OJMILWh6ARY)\n",
    "- Run the following codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ensemble Methods\n",
    "---\n",
    "1. <b>Random Forest</b>: Bagging + Decision Tree\n",
    "2. <b>Boosting</b>: Sequantial bagging + any type of supervised learning methods (but mostly with Decision Tree)\n",
    "    - <b>AdaBoost (Adaptive Boosting)</b>: At every iteration of bagging, the Adaboost weights more those cases that were misclassified in the previous classification and updates the classifier towards improving the accuracy.\n",
    "    - <b>Gradient boosting</b>: At every iteration of training procedure, it fits the classifier model to the residual that could not be predicted in the previous regression. As such, the algorithim gradually reduces the residual between observation and prediction, providing increasingly higher prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Adaboost\n",
    "---\n",
    "## Differences with Random Forest\n",
    "- Flexible depth of decision trees vs. Fixed size of stump\n",
    "- Independent tree training vs. Sequential tree imporving model\n",
    "- Multiple randomly selected features vs. One feature at a time\n",
    "- Equal sample weight vs. different weight depending on the prediction results\n",
    "\n",
    "## Keywords\n",
    "- How to build the stump? -> Based on CALT + Gini index! (Decision Tree Algorithm)\n",
    "- How to compute the \"Amount of Say\"\n",
    "- How to update the weight of correctly classified and misclassified samples\n",
    "- How to resample the cases for the next stump -> Bootstrap of weighted samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Heart disease classification\n",
    "---\n",
    "<img src='https://raw.githubusercontent.com/COMBINE-SKKU/combine-skku/master/class/week6/Fig6-1.png' width=\"500\"/>\n",
    "<br>\n",
    "1. How to build the stump? Which feature are we going to use to build a first stump?: CALT cost function + Gini index\n",
    "<img src='https://raw.githubusercontent.com/COMBINE-SKKU/combine-skku/master/class/week6/Fig6-2.png' width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Heart disease classification\n",
    "---\n",
    "    Answer: The CALT cost function selected the income feature with $51509 of a threshold as the best criterion to predict the illness condition. \n",
    "<img src='https://raw.githubusercontent.com/COMBINE-SKKU/combine-skku/master/class/week6/Fig6-3.png' width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. How to compute the \"Amount of Say\"? \n",
    "<img src='https://raw.githubusercontent.com/COMBINE-SKKU/combine-skku/master/class/week6/Fig6-4.png' width=\"500\"/>\n",
    "Answer: Amount of Say for Income (threshold: 51509): <b>1/2 * log((1-(2/8))/(2/8))=0.23</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3. How to update the weight of correctly classified and misclassified samples?\n",
    "<img src='https://raw.githubusercontent.com/COMBINE-SKKU/combine-skku/master/class/week6/Fig6-5.png' width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='https://raw.githubusercontent.com/COMBINE-SKKU/combine-skku/master/class/week6/Fig6-6.png' width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "4. How to resample the cases for the next stump?\n",
    "<img src='https://raw.githubusercontent.com/COMBINE-SKKU/combine-skku/master/class/week6/Fig6-7.png' width=\"500\"/>\n",
    "Answer: Resampling more those cases with a higher weight. By doing this, the training of Adaboost can be more adjusted towards previously mis-classified samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='https://raw.githubusercontent.com/COMBINE-SKKU/combine-skku/master/class/week6/Fig6-8.png' width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "5. Iterate the entire procedure 1)-4) again for next stumps building until the total error rate becomes unchanged as a minimum.\n",
    "<img src='https://raw.githubusercontent.com/COMBINE-SKKU/combine-skku/master/class/week6/Fig6-9.png' width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Building an Adaboost model in Python\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import datasets\n",
    "\n",
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Load Iris data\n",
    "# This dataset comprises 4 features \n",
    "# (sepal length, sepal width, petal length, \n",
    "# petal width) and a target (the type of flower).\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create adaboost classifer object\n",
    "abc = AdaBoostClassifier(n_estimators=50, learning_rate=1) \n",
    "# Learning rate shrinks the contribution of each classifier by learning_rate. \n",
    "# There is a trade-off between learning_rate and n_estimators.\n",
    "\n",
    "# Train Adaboost Classifer\n",
    "model = abc.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using Different Base Learners\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "# Import Support Vector Classifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "svc=SVC(probability=True, kernel='linear')\n",
    "\n",
    "# Create adaboost classifer object\n",
    "abc =AdaBoostClassifier(n_estimators=50, base_estimator=svc, learning_rate=1)\n",
    "\n",
    "# Train Adaboost Classifer\n",
    "model = abc.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pros\n",
    "---\n",
    "AdaBoost is easy to implement. It iteratively corrects the mistakes of the weak classifier and improves accuracy by combining weak learners. You can use many base classifiers with AdaBoost. AdaBoost is not prone to overfitting. This can be found out via experiment results, but there is no concrete reason available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Cons\n",
    "---\n",
    "AdaBoost is sensitive to noise data. It is highly affected by outliers because it tries to fit each point perfectly. AdaBoost is slower compared to XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Building a Gradient Boosting model in Python\n",
    "---\n",
    "- Another very popular boosting algorithm\n",
    "- Instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the residual errors made by the previous predictor.\n",
    "- Compared to AdaBoost, it introduced the ideas from bootstrap aggregation to further improve the models, such as randomly sampling the data as well as features (like random forest) when fitting ensemble members.\n",
    "- Models are fit using any arbitrary differentiable loss function and <b>gradient descent optimization algorithm</b>. This gives the technique its name, “gradient boosting,” as the loss gradient is minimized as the model is fit, much like a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22.1\n",
      "(1000, 20) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# check scikit-learn version\n",
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "\n",
    "# test regression dataset\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=7)\n",
    "\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: -62.463 (3.233)\n"
     ]
    }
   ],
   "source": [
    "# evaluate gradient boosting ensemble for regression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# define the model\n",
    "model = GradientBoostingRegressor()\n",
    "\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# evaluate the model\n",
    "n_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "# report performance\n",
    "print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to set up the hyperparameters\n",
    "---\n",
    "- Gradient boosting can be challenging to configure as the algorithm as many key hyperparameters that influence the behavior of the model on training data and the hyperparameters interact with each other.\n",
    "- Popular search processes include a random search and a grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# example of grid searching key hyperparameters for gradient boosting on a classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "\n",
    "# define the model with default hyperparameters\n",
    "model = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.938000 using {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.7}\n",
      "0.531333 (0.095070) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.525333 (0.077060) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.524000 (0.072874) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.771667 (0.032154) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.772333 (0.038874) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.738667 (0.049982) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.827000 (0.031953) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.814000 (0.037292) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.761000 (0.043077) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.829667 (0.037281) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 500, 'subsample': 0.5}\n",
      "0.814000 (0.043558) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 500, 'subsample': 0.7}\n",
      "0.773667 (0.034975) with: {'learning_rate': 0.0001, 'max_depth': 3, 'n_estimators': 500, 'subsample': 1.0}\n",
      "0.536333 (0.109163) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.535333 (0.106136) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.533000 (0.099437) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.831667 (0.035692) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.838667 (0.029747) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.804333 (0.032216) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.870000 (0.029326) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.870667 (0.031084) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.809333 (0.029545) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.889000 (0.031765) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.5}\n",
      "0.876667 (0.029364) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.7}\n",
      "0.810333 (0.032709) with: {'learning_rate': 0.0001, 'max_depth': 7, 'n_estimators': 500, 'subsample': 1.0}\n",
      "0.537000 (0.111090) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.535000 (0.105601) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.531667 (0.095012) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.842000 (0.032701) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.845000 (0.028607) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.806000 (0.029280) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.873000 (0.028419) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.877333 (0.027801) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.812000 (0.025351) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.892000 (0.029710) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 500, 'subsample': 0.5}\n",
      "0.881000 (0.030914) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 500, 'subsample': 0.7}\n",
      "0.812000 (0.029484) with: {'learning_rate': 0.0001, 'max_depth': 9, 'n_estimators': 500, 'subsample': 1.0}\n",
      "0.813667 (0.031250) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.811333 (0.037035) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.761333 (0.043107) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.828333 (0.037690) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.812667 (0.041226) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.773667 (0.034975) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.835333 (0.040966) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.814000 (0.039716) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.781333 (0.034325) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.845333 (0.033340) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 500, 'subsample': 0.5}\n",
      "0.832667 (0.037411) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 500, 'subsample': 0.7}\n",
      "0.814333 (0.032730) with: {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 500, 'subsample': 1.0}\n",
      "0.857333 (0.025682) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.853667 (0.029153) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.809000 (0.031236) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.877333 (0.030434) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.876333 (0.028223) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.810667 (0.033160) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.885667 (0.033034) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.878667 (0.029747) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.809667 (0.031356) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.894333 (0.032216) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.5}\n",
      "0.885667 (0.027771) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.7}\n",
      "0.823000 (0.020355) with: {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 500, 'subsample': 1.0}\n",
      "0.855000 (0.032838) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.853667 (0.033013) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.808667 (0.027047) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.888000 (0.027495) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.884000 (0.025768) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.812333 (0.028247) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.892667 (0.028628) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.884333 (0.028482) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.812667 (0.026196) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.902000 (0.027857) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 500, 'subsample': 0.5}\n",
      "0.891000 (0.026752) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 500, 'subsample': 0.7}\n",
      "0.818333 (0.028992) with: {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 500, 'subsample': 1.0}\n",
      "0.824333 (0.040388) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.809667 (0.041431) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.780000 (0.035214) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.844000 (0.033724) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.832000 (0.038070) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.813333 (0.032283) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.854333 (0.029176) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.847667 (0.030076) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.836000 (0.034020) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.896000 (0.029052) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500, 'subsample': 0.5}\n",
      "0.891333 (0.031489) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500, 'subsample': 0.7}\n",
      "0.878333 (0.029562) with: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500, 'subsample': 1.0}\n",
      "0.865000 (0.036125) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.869667 (0.031988) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.808333 (0.031526) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.891667 (0.028294) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.882000 (0.029029) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.822667 (0.025157) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.896000 (0.026026) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.890333 (0.028575) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.837667 (0.026418) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.924333 (0.026036) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.5}\n",
      "0.920000 (0.029326) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.7}\n",
      "0.883000 (0.032368) with: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 500, 'subsample': 1.0}\n",
      "0.871333 (0.031700) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.869000 (0.027970) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.811000 (0.029816) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.890333 (0.028459) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.887000 (0.028537) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.817667 (0.031164) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.901333 (0.026043) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.892000 (0.027495) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.820333 (0.029381) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.924333 (0.028599) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 500, 'subsample': 0.5}\n",
      "0.918667 (0.027897) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 500, 'subsample': 0.7}\n",
      "0.831333 (0.027657) with: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 500, 'subsample': 1.0}\n",
      "0.852667 (0.032035) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.840333 (0.036284) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.830000 (0.036515) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.886333 (0.034591) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.887667 (0.026793) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.880667 (0.034052) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.901667 (0.030009) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.901333 (0.027657) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.898667 (0.031063) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.928333 (0.023956) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'subsample': 0.5}\n",
      "0.927667 (0.026418) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'subsample': 0.7}\n",
      "0.918667 (0.026043) with: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500, 'subsample': 1.0}\n",
      "0.873000 (0.028419) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.872333 (0.027891) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.833000 (0.027707) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.912000 (0.026633) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.911333 (0.026297) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.881667 (0.032361) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.930000 (0.024358) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.923333 (0.026750) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.907000 (0.025710) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.935000 (0.022174) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.5}\n",
      "0.938000 (0.023007) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.7}\n",
      "0.911667 (0.030341) with: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 500, 'subsample': 1.0}\n",
      "0.879667 (0.037102) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.876667 (0.028087) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.823333 (0.026247) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.910333 (0.025882) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.910667 (0.028040) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.843333 (0.032796) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.924667 (0.024184) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.929667 (0.026518) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.850333 (0.033315) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.937667 (0.024588) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 500, 'subsample': 0.5}\n",
      "0.931000 (0.027970) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 500, 'subsample': 0.7}\n",
      "0.859333 (0.033954) with: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 500, 'subsample': 1.0}\n",
      "0.825667 (0.045364) with: {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.840000 (0.043818) with: {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.848333 (0.029107) with: {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.817667 (0.040881) with: {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.864333 (0.034514) with: {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.895667 (0.033034) with: {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.824000 (0.031791) with: {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.886000 (0.033823) with: {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.907333 (0.025682) with: {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.819000 (0.037090) with: {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 500, 'subsample': 0.5}\n",
      "0.880333 (0.038339) with: {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 500, 'subsample': 0.7}\n",
      "0.908333 (0.025441) with: {'learning_rate': 1.0, 'max_depth': 3, 'n_estimators': 500, 'subsample': 1.0}\n",
      "0.811667 (0.035598) with: {'learning_rate': 1.0, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.840000 (0.034351) with: {'learning_rate': 1.0, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.878667 (0.032325) with: {'learning_rate': 1.0, 'max_depth': 7, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.845000 (0.044926) with: {'learning_rate': 1.0, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.887667 (0.040717) with: {'learning_rate': 1.0, 'max_depth': 7, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.893667 (0.029381) with: {'learning_rate': 1.0, 'max_depth': 7, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.828667 (0.050513) with: {'learning_rate': 1.0, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.889667 (0.030494) with: {'learning_rate': 1.0, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.894667 (0.032836) with: {'learning_rate': 1.0, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.721333 (0.132407) with: {'learning_rate': 1.0, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.5}\n",
      "0.896667 (0.031127) with: {'learning_rate': 1.0, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.7}\n",
      "0.896000 (0.028472) with: {'learning_rate': 1.0, 'max_depth': 7, 'n_estimators': 500, 'subsample': 1.0}\n",
      "0.813000 (0.027099) with: {'learning_rate': 1.0, 'max_depth': 9, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.860667 (0.036234) with: {'learning_rate': 1.0, 'max_depth': 9, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.876333 (0.026644) with: {'learning_rate': 1.0, 'max_depth': 9, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.840000 (0.042973) with: {'learning_rate': 1.0, 'max_depth': 9, 'n_estimators': 50, 'subsample': 0.5}\n",
      "0.889667 (0.031036) with: {'learning_rate': 1.0, 'max_depth': 9, 'n_estimators': 50, 'subsample': 0.7}\n",
      "0.878667 (0.027293) with: {'learning_rate': 1.0, 'max_depth': 9, 'n_estimators': 50, 'subsample': 1.0}\n",
      "0.829333 (0.066880) with: {'learning_rate': 1.0, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.890333 (0.022874) with: {'learning_rate': 1.0, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.885667 (0.026418) with: {'learning_rate': 1.0, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.784333 (0.129246) with: {'learning_rate': 1.0, 'max_depth': 9, 'n_estimators': 500, 'subsample': 0.5}\n",
      "0.882667 (0.027921) with: {'learning_rate': 1.0, 'max_depth': 9, 'n_estimators': 500, 'subsample': 0.7}\n",
      "0.884333 (0.029061) with: {'learning_rate': 1.0, 'max_depth': 9, 'n_estimators': 500, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# define the grid of values to search\n",
    "grid = dict()\n",
    "grid['n_estimators'] = [10, 50, 100, 500]\n",
    "grid['learning_rate'] = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "grid['subsample'] = [0.5, 0.7, 1.0]\n",
    "grid['max_depth'] = [3, 7, 9]\n",
    "\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# define the grid search procedure\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy')\n",
    "\n",
    "# execute the grid search\n",
    "grid_result = grid_search.fit(X, y)\n",
    "\n",
    "# summarize the best score and configuration\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "# summarize all scores that were evaluated\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
